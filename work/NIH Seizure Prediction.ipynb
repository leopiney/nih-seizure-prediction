{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Seizure Onset in Epileptic Patients Using Intracranial EEG Recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress\n",
    "\n",
    "An [interesting article](http://cs229.stanford.edu/proj2014/Janet%20An,%20Amy%20Bearman,%20Catherine%20Dong,%20Predicting%20Seizure%20Onset%20in%20Epileptic%20Patients%20Using%20Intercranial%20EEG%20Recordings.pdf) to start working with. It hasn't many details on implementation, but gives some ideas of what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data scientist weapons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.io\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "debug_here = Tracer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a balanced set of Interictal (Non-Seizure, 0) and Preictal (Pre-Seizure, 1) examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir_train = u'/data/train_1/'\n",
    "base_dir_tests = u'/data/test_1/'\n",
    "\n",
    "INTERICTAL = 0\n",
    "PREICTAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_class_from_name(name):\n",
    "    \"\"\"\n",
    "    Gets the class from the file name.\n",
    "    \n",
    "    The class is defined by the last number written in the file name.\n",
    "    For example:\n",
    "    \n",
    "    Input: \".../1_1_1.mat\"\n",
    "    Output: 1.0\n",
    "    \n",
    "    Input: \".../1_1_0.mat\"\n",
    "    Output: 0.0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(name[-5])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "assert get_class_from_name('/train_1/1_1_0.mat') == 0.0\n",
    "assert get_class_from_name('/train_1/1_1_1.mat') == 1.0\n",
    "\n",
    "\n",
    "def get_file_names_and_classes(base_dir):\n",
    "    ignored_files = ['.DS_Store', '1_45_1.mat']\n",
    "    \n",
    "    return np.array(\n",
    "        [\n",
    "            (file, get_class_from_name(file)) \n",
    "            for file in os.listdir(base_dir) if file not in ignored_files\n",
    "        ],\n",
    "        dtype=[('file', '|S16'), ('class', 'float32')]\n",
    "    )\n",
    "\n",
    "data_files_all = get_file_names_and_classes(base_dir_train)\n",
    "\n",
    "# Count the occurrences of Interictal and Preictal classes\n",
    "unique, counts = np.unique(data_files_all['class'], return_counts=True)\n",
    "occurrences = dict(zip(unique, counts))\n",
    "\n",
    "print('Interictal samples:', occurrences.get(INTERICTAL))\n",
    "print('Preictal samples:', occurrences.get(PREICTAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If set to true, it will get a balanced set of Interictal and Preictal samples.\n",
    "use_equal_size_sets = False\n",
    "\n",
    "if use_equal_size_sets:\n",
    "    set_size = 150\n",
    "\n",
    "    # Randomly select an equal-size set of Interictal and Preictal samples\n",
    "    data_random_interictal = np.random.choice(data_files_all[data_files_all['class'] == 0], size=set_size)\n",
    "    data_random_preictal = np.random.choice(data_files_all[data_files_all['class'] == 1], size=set_size)\n",
    "\n",
    "    # Merge the data sets and shufle the collection\n",
    "    data_files = np.concatenate([data_random_interictal, data_random_preictal])\n",
    "    data_files.dtype = data_files_all.dtype  # Sets the same dtype than the original collection\n",
    "else:\n",
    "    data_files = data_files_all\n",
    "\n",
    "print('Data files shape:', data_files.shape, 'Data files size:', data_files.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from .mat files and pre-process\n",
    "\n",
    "Load data for each file and get the definitive training data.\n",
    "\n",
    "Then create the `y` based on the classes of the files.\n",
    "Note that because each file contains 16 channels, it is neccesary to\n",
    "repeat each class in data_files['class'] 16 times, respecting the order of appearence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def perform_correlation(pd_data, corr_type):\n",
    "    \"\"\"\n",
    "    Performs the correlation of type `corr_type` over the `pd_data`\n",
    "    The parameter `pd_data` is a Pandas DataFrame\n",
    "    \"\"\"\n",
    "    C = np.array(pd_data.corr(corr_type))\n",
    "\n",
    "    # Sets all NaN and infinite values to 0\n",
    "    C[np.isnan(C)] = 0\n",
    "    C[np.isinf(C)] = 0\n",
    "\n",
    "    # Calculates the Eigen values and vectors with the Numpy's Linear Algebra library\n",
    "    w, v = LA.eig(C)\n",
    "\n",
    "    # Sorts and casts all the values as real numbers\n",
    "    w.sort()\n",
    "    return w.real\n",
    "\n",
    "\n",
    "def get_shannon_entropy(probs):\n",
    "    \"\"\"\n",
    "    Given an array of probabilities of certain elements, returns the Shannon entropy\n",
    "    \"\"\"\n",
    "    return -1 * np.sum(np.multiply(probs, np.log2(probs)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "\n",
    "def get_X_from_files(base_dir, files, show_progress=True):\n",
    "    \"\"\"\n",
    "    Given a list of filenames, returns the final data we want to train the models.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    total_files = len(files)\n",
    "\n",
    "    for i, filename in enumerate(files):\n",
    "        if show_progress and i % int(total_files / 10) == 0:\n",
    "            print(u'%{}: Loading file {}'.format(int(i * 100 / total_files), filename))\n",
    "\n",
    "        try:\n",
    "            mat_data = scipy.io.loadmat(''.join([base_dir, filename.decode('UTF-8')]))\n",
    "        except ValueError as ex:\n",
    "            print(u'Error loading MAT file {}: {}'.format(filename, str(ex)))\n",
    "            continue\n",
    "\n",
    "        # Gets a 16x240000 matrix => 16 channels reading data for 10 minutes at 400Hz\n",
    "        egg_data = mat_data['dataStruct']['data'][0, 0]\n",
    "        fs = mat_data['dataStruct']['iEEGsamplingRate'][0, 0][0, 0]\n",
    "        \n",
    "        # The shape of the EGG data\n",
    "        n_time, n_channels = egg_data.shape\n",
    "        \n",
    "        # Define the window sample length. In this case is the number of meassurements in one minute\n",
    "        samp_len = math.floor(fs * 60)\n",
    "        samps_count = int(math.floor(n_time / samp_len))\n",
    "        \n",
    "        # The index of the samples in the whole EEG reading data\n",
    "        samp_index = range(0, (samps_count + 1) * samp_len, samp_len)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for i in range(samps_count):\n",
    "            epoch = egg_data[samp_index[i]: samp_index[i + 1], :]\n",
    "            \n",
    "            #\n",
    "            # Compute Shannon's entropy, spectral edge and correlation matrix\n",
    "            # segments corresponding to frequency bands\n",
    "            #\n",
    "            lvl = np.array([0.1, 4, 8, 12, 30, 70, 180])  # Frequency levels in Hz\n",
    "            lseg = np.round(n_time / fs * lvl).astype('int')\n",
    "            \n",
    "            # Calculates the Fourier transformation under the period from 0 to n_time / fs * lvl[-1]\n",
    "            D = np.absolute(np.fft.fft(epoch, n=lseg[-1], axis=0))\n",
    "            D[0, :] = 0  # set the DC component to zero\n",
    "            D /= D.sum()  # Normalize each channel\n",
    "            \n",
    "            dspect = np.zeros((len(lvl) - 1, n_channels))\n",
    "            for j in range(len(dspect)):\n",
    "                dspect[j, :] = 2 * np.sum(D[lseg[j]: lseg[j+1], :], axis=0)\n",
    "            \n",
    "            # Gets the shannon's entropy\n",
    "            shannon_entropy = get_shannon_entropy(dspect)\n",
    "            \n",
    "            #\n",
    "            # Find the spectral edge frequency\n",
    "            #\n",
    "            sfreq = fs\n",
    "            tfreq = 40\n",
    "            ppow = 0.5\n",
    "            \n",
    "            topfreq = int(round(n_time / sfreq * tfreq)) + 1\n",
    "            A = np.cumsum(D[:topfreq, :])\n",
    "            B = A - A.max() * ppow\n",
    "            spedge = np.min(np.abs(B))\n",
    "            spedge = (spedge - 1) / (topfreq - 1) * tfreq\n",
    "            \n",
    "            # Calculate correlation matrix and its eigenvalues (b/w channels)\n",
    "            data = pd.DataFrame(data=epoch)\n",
    "            pearson_lxchannels = perform_correlation(data, 'pearson')\n",
    "            spearman_lxchannels = perform_correlation(data, 'spearman')\n",
    "\n",
    "            # Calculate correlation matrix and its eigenvalues (b/w freq)\n",
    "            data = pd.DataFrame(data=dspect)\n",
    "            pearson_lxfreqbands = perform_correlation(data, 'pearson')\n",
    "            spearman_lxfreqbands = perform_correlation(data, 'spearman')\n",
    "            \n",
    "            #\n",
    "            # Spectral entropy for dyadic bands\n",
    "            #\n",
    "\n",
    "            # Find number of dyadic levels\n",
    "            ldat = int(math.floor(n_time / 2.0))\n",
    "            no_levels = int(math.floor(math.log(ldat, 2.0)))\n",
    "            seg = math.floor(ldat / pow(2.0, no_levels - 1))\n",
    "\n",
    "            # Find the power spectrum at each dyadic level\n",
    "            dspect = np.zeros((no_levels, n_channels))\n",
    "            for j in range(no_levels-1,-1,-1):\n",
    "                dspect[j,:] = 2 * np.sum(D[int(math.floor(ldat / 2.0)) + 1: ldat, :], axis=0)\n",
    "                ldat = int(math.floor(ldat / 2.0))\n",
    "\n",
    "            # Find the Shannon's entropy\n",
    "            shannon_entropy_dyd = get_shannon_entropy(dspect)\n",
    "\n",
    "            # Find correlation between channels\n",
    "            pearson_lxchannels_dyd = perform_correlation(pd.DataFrame(data=dspect), 'pearson')\n",
    "            spearman_lxchannels_dyd = perform_correlation(pd.DataFrame(data=dspect), 'spearman')\n",
    "            \n",
    "            #\n",
    "            # Hjorth parameters\n",
    "            #\n",
    "\n",
    "            # Activity\n",
    "            activity = np.var(epoch, axis=0)\n",
    "\n",
    "            # Mobility\n",
    "            mobility = np.divide(\n",
    "                np.std(np.diff(epoch, axis=0)), \n",
    "                np.std(epoch, axis=0)\n",
    "            )\n",
    "\n",
    "            #\n",
    "            # Complexity\n",
    "            # std of second derivative for each channel\n",
    "            #\n",
    "            complexity = np.divide(\n",
    "                np.divide(\n",
    "                    np.std(np.diff(np.diff(epoch, axis=0), axis=0), axis=0),\n",
    "                    np.std(np.diff(epoch, axis=0), axis=0)\n",
    "                ),\n",
    "                mobility\n",
    "            )\n",
    "            \n",
    "            #\n",
    "            # Statistical properties\n",
    "            #\n",
    "            \n",
    "            # Skewness\n",
    "            sk = stats.skew(epoch)\n",
    "\n",
    "            # Kurtosis\n",
    "            kurt = stats.kurtosis(epoch)\n",
    "            \n",
    "            features = np.concatenate((\n",
    "                features,\n",
    "                shannon_entropy.ravel(),\n",
    "                spedge.ravel(),\n",
    "                pearson_lxchannels.ravel(),\n",
    "                pearson_lxfreqbands.ravel(),\n",
    "                spearman_lxchannels.ravel(),\n",
    "                spearman_lxfreqbands.ravel(),\n",
    "                shannon_entropy_dyd.ravel(),\n",
    "                pearson_lxchannels_dyd.ravel(),\n",
    "                spearman_lxchannels_dyd.ravel(),\n",
    "                activity.ravel(),\n",
    "                mobility.ravel(),\n",
    "                complexity.ravel(),\n",
    "                sk.ravel(),\n",
    "                kurt.ravel(),\n",
    "            ))\n",
    "            \n",
    "        X.append(features)\n",
    "\n",
    "    X = np.array(X)\n",
    "    X[np.isnan(X)] = 0\n",
    "    X[np.isinf(X)] = 0\n",
    "    return X.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time X = get_X_from_files(base_dir_train, data_files['file'], show_progress=True)\n",
    "y = data_files['class']\n",
    "\n",
    "print('X shape:', X.shape, 'X size:', X.size)\n",
    "print('y shape:', y.shape, 'y size:', y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Resamblation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalizes the data\n",
    "normalize(X, copy=False)\n",
    "\n",
    "# Plots a user normalized sample\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 5.0)\n",
    "print('Showing case of file:', data_files['file'][0])\n",
    "for i in range(16):\n",
    "    plt.subplot(8, 2, i + 1)\n",
    "    plt.plot(X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "for n, t in zip(['X_train', 'X_test', 'y_train', 'y_test'], [X_train, X_test, y_train, y_test]):\n",
    "    print('{} shape'.format(n), t.shape, '{} size'.format(n), t.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "results = {}\n",
    "\n",
    "for C in [10, 100, 500, 750, 950, 1000, 1050, 1250, 1500, 10000]:\n",
    "    print('Creating model with C={}'.format(C))\n",
    "\n",
    "    clf = linear_model.LogisticRegression(C=C, n_jobs=4, solver='liblinear', verbose=10)\n",
    "\n",
    "    %time clf.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "    %time y_pred = clf.predict(X_test)\n",
    "\n",
    "    results[C] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred),\n",
    "    }\n",
    "    \n",
    "    for key, score in results[C].items():\n",
    "        print('{}: {}'.format(key, score))\n",
    "        \n",
    "data = [(C, score['roc_auc']) for C, score in results.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data)\n",
    "plt.plot([d[1] for d in sorted(data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "60 samples (50/50)\n",
    "-------------------\n",
    "\n",
    "(C=.1, n_jobs=1, solver='liblinear')\n",
    "```\n",
    "CPU times: user 0 ns, sys: 10 ms, total: 10 ms\n",
    "Wall time: 5.49 ms\n",
    "Accuracy: 0.637223974763\n",
    "Precision: 0.627218934911\n",
    "Recall: 0.670886075949\n",
    "F1 score: 0.648318042813\n",
    "```\n",
    "\n",
    "120 samples (50/50)\n",
    "-------------------\n",
    "\n",
    "(C=.1, n_jobs=1, solver='liblinear')\n",
    "\n",
    "```\n",
    "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
    "Wall time: 6.98 ms\n",
    "Accuracy: 0.61356466877\n",
    "Precision: 0.611111111111\n",
    "Recall: 0.597444089457\n",
    "F1 score: 0.604200323102\n",
    "```\n",
    "\n",
    "(C=1, n_jobs=1, solver='liblinear')\n",
    "\n",
    "```\n",
    "CPU times: user 0 ns, sys: 20 ms, total: 20 ms\n",
    "Wall time: 10.9 ms\n",
    "Accuracy: 0.652996845426\n",
    "Precision: 0.650485436893\n",
    "Recall: 0.642172523962\n",
    "F1 score: 0.646302250804\n",
    "```\n",
    "\n",
    "(C=10, n_jobs=1, solver='liblinear')\n",
    "\n",
    "```\n",
    "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
    "Wall time: 7.13 ms\n",
    "Accuracy: 0.708201892744\n",
    "Precision: 0.703821656051\n",
    "Recall: 0.70607028754\n",
    "F1 score: 0.704944178628\n",
    "```\n",
    "\n",
    "238 samples\n",
    "-----------\n",
    "\n",
    "(C=16, n_jobs=1, solver='liblinear')\n",
    "\n",
    "```\n",
    "CPU times: user 40 ms, sys: 0 ns, total: 40 ms\n",
    "Wall time: 21 ms\n",
    "Accuracy: 0.695679796696\n",
    "Precision: 0.684848484848\n",
    "Recall: 0.720663265306\n",
    "F1 score: 0.702299564947\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

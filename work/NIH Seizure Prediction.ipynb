{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Seizure Onset in Epileptic Patients Using Intracranial EEG Recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress\n",
    "\n",
    "An [interesting article](http://cs229.stanford.edu/proj2014/Janet%20An,%20Amy%20Bearman,%20Catherine%20Dong,%20Predicting%20Seizure%20Onset%20in%20Epileptic%20Patients%20Using%20Intercranial%20EEG%20Recordings.pdf) to start working with. It hasn't many details on implementation, but gives some ideas of what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data scientist weapons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "debug_here = Tracer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a balanced set of Interictal (Non-Seizure, 0) and Preictal (Pre-Seizure, 1) examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir_train = u'/data/train_1/'\n",
    "base_dir_tests = u'/data/test_1/'\n",
    "\n",
    "INTERICTAL = 0\n",
    "PREICTAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interictal samples: 1152\n",
      "Preictal samples: 149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_class_from_name(name):\n",
    "    \"\"\"\n",
    "    Gets the class from the file name.\n",
    "    \n",
    "    The class is defined by the last number written in the file name.\n",
    "    For example:\n",
    "    \n",
    "    Input: \".../1_1_1.mat\"\n",
    "    Output: 1.0\n",
    "    \n",
    "    Input: \".../1_1_0.mat\"\n",
    "    Output: 0.0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(name[-5])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "assert get_class_from_name('/train_1/1_1_0.mat') == 0.0\n",
    "assert get_class_from_name('/train_1/1_1_1.mat') == 1.0\n",
    "\n",
    "\n",
    "def get_file_names_and_classes(base_dir):\n",
    "    ignored_files = ['.DS_Store', '1_45_1.mat']\n",
    "    \n",
    "    return np.array(\n",
    "        [\n",
    "            (file, get_class_from_name(file)) \n",
    "            for file in os.listdir(base_dir) if file not in ignored_files\n",
    "        ],\n",
    "        dtype=[('file', '|S16'), ('class', 'float32')]\n",
    "    )\n",
    "\n",
    "data_files_all = get_file_names_and_classes(base_dir_train)\n",
    "\n",
    "# Count the occurrences of Interictal and Preictal classes\n",
    "unique, counts = np.unique(data_files_all['class'], return_counts=True)\n",
    "occurrences = dict(zip(unique, counts))\n",
    "\n",
    "print('Interictal samples:', occurrences.get(INTERICTAL))\n",
    "print('Preictal samples:', occurrences.get(PREICTAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files shape: (300,) Data files size: 300\n"
     ]
    }
   ],
   "source": [
    "# If set to true, it will get a balanced set of Interictal and Preictal samples.\n",
    "use_equal_size_sets = True\n",
    "\n",
    "if use_equal_size_sets:\n",
    "    set_size = 150\n",
    "\n",
    "    # Randomly select an equal-size set of Interictal and Preictal samples\n",
    "    data_random_interictal = np.random.choice(data_files_all[data_files_all['class'] == 0], size=set_size)\n",
    "    data_random_preictal = np.random.choice(data_files_all[data_files_all['class'] == 1], size=set_size)\n",
    "\n",
    "    # Merge the data sets and shufle the collection\n",
    "    data_files = np.concatenate([data_random_interictal, data_random_preictal])\n",
    "    data_files.dtype = data_files_all.dtype  # Sets the same dtype than the original collection\n",
    "else:\n",
    "    data_files = data_files_all\n",
    "\n",
    "print('Data files shape:', data_files.shape, 'Data files size:', data_files.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from .mat files and pre-process\n",
    "\n",
    "Load data for each file and get the definitive training data.\n",
    "\n",
    "Then create the `y` based on the classes of the files.\n",
    "Note that because each file contains 16 channels, it is neccesary to\n",
    "repeat each class in data_files['class'] 16 times, respecting the order of appearence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def perform_correlation(pd_data, corr_type):\n",
    "    \"\"\"\n",
    "    Performs the correlation of type `corr_type` over the `pd_data`\n",
    "    The parameter `pd_data` is a Pandas DataFrame\n",
    "    \"\"\"\n",
    "    C = np.array(pd_data.corr(corr_type))\n",
    "\n",
    "    # Sets all NaN and infinite values to 0\n",
    "    C[np.isnan(C)] = 0\n",
    "    C[np.isinf(C)] = 0\n",
    "\n",
    "    # Calculates the Eigen values and vectors with the Numpy's Linear Algebra library\n",
    "    w, v = LA.eig(C)\n",
    "\n",
    "    # Sorts and casts all the values as real numbers\n",
    "    w.sort()\n",
    "    return w.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from scipy.signal import correlate, resample\n",
    "\n",
    "\n",
    "def get_X_from_files(base_dir, files, show_progress=True):\n",
    "    \"\"\"\n",
    "    Given a list of filenames, returns the final data we want to train the models.\n",
    "    \"\"\"\n",
    "    X = None\n",
    "\n",
    "    total_files = len(files)\n",
    "\n",
    "    for i, filename in enumerate(files):\n",
    "        if show_progress and i % int(total_files / 10) == 0:\n",
    "            print(u'%{}: Loading file {}'.format(int(i * 100 / total_files), filename))\n",
    "\n",
    "        try:\n",
    "            mat_data = scipy.io.loadmat(''.join([base_dir, filename.decode('UTF-8')]))\n",
    "        except ValueError as ex:\n",
    "            print(u'Error loading MAT file {}: {}'.format(filename, str(ex)))\n",
    "            continue\n",
    "\n",
    "        # Gets a 16x240000 matrix => 16 channels reading data for 10 minutes at 400Hz\n",
    "        channels_data = mat_data['dataStruct']['data'][0, 0].transpose()\n",
    "\n",
    "        # Resamble each channel to get only a meassurement per second\n",
    "        # 10 minutes of measurements, grouping data on each second\n",
    "        channels_data = resample(channels_data, 120, axis=1, window=2000)\n",
    "\n",
    "        # It seems that adding bivariate meassurements helps a lot on\n",
    "        # signals pattern recognition.\n",
    "        # For each channel, add the correlation meassurements with all the other\n",
    "        # channels.\n",
    "        # TODO: This should be done in a more efficient way ¯\\_(ツ)_/¯\n",
    "        correlations = None\n",
    "        for i in range(16):\n",
    "            correlations_i = np.array([])\n",
    "            for j in range (16):\n",
    "                if i != j:\n",
    "                    corr_i = correlate(channels_data[i], channels_data[j], mode='same')\n",
    "                    correlations_i = np.concatenate([correlations_i, corr_i])\n",
    "\n",
    "            if correlations is None:\n",
    "                correlations = correlations_i\n",
    "            else:\n",
    "                correlations = np.vstack([correlations, correlations_i])\n",
    "\n",
    "        channels_data = np.column_stack([channels_data, correlations])\n",
    "\n",
    "        X = np.vstack([X, channels_data]) if X is not None else channels_data\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%0: Loading file b'1_669_0.mat'\n",
      "%10: Loading file b'1_1147_0.mat'\n",
      "%20: Loading file b'1_448_0.mat'\n",
      "%30: Loading file b'1_970_0.mat'\n",
      "%40: Loading file b'1_653_0.mat'\n",
      "%50: Loading file b'1_120_1.mat'\n",
      "%60: Loading file b'1_110_1.mat'\n",
      "%70: Loading file b'1_136_1.mat'\n",
      "%80: Loading file b'1_108_1.mat'\n",
      "%90: Loading file b'1_118_1.mat'\n",
      "CPU times: user 50.3 s, sys: 8.07 s, total: 58.4 s\n",
      "Wall time: 1min\n",
      "X shape: (4800, 1920) X size: 9216000\n",
      "y shape: (4800,) y size: 4800\n"
     ]
    }
   ],
   "source": [
    "%time X = get_X_from_files(base_dir_train, data_files['file'])\n",
    "y = np.repeat(data_files['class'], 16, axis=0)  # one tag for each channel\n",
    "\n",
    "print('X shape:', X.shape, 'X size:', X.size)\n",
    "print('y shape:', y.shape, 'y size:', y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data', 'iEEGsamplingRate', 'nSamplesSegment', 'channelIndices', 'sequence')\n",
      "(240000, 16)\n"
     ]
    }
   ],
   "source": [
    "mat_data = scipy.io.loadmat(''.join([base_dir_train, '1_1_1.mat']))\n",
    "names = mat_data['dataStruct'].dtype.names\n",
    "ndata = {n: mat_data['dataStruct'][n][0, 0] for n in names}\n",
    "\n",
    "# print(mat_data['dataStruct']['data'][0, 0])\n",
    "print(names)\n",
    "print(mat_data['dataStruct']['data'][0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Resamblation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalizes the data\n",
    "normalize(X, copy=False)\n",
    "\n",
    "# Plots a user normalized sample\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 5.0)\n",
    "print('Showing case of file:', data_files['file'][0])\n",
    "for i in range(16):\n",
    "    plt.subplot(8, 2, i + 1)\n",
    "    plt.plot(X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "for n, t in zip(['X_train', 'X_test', 'y_train', 'y_test'], [X_train, X_test, y_train, y_test]):\n",
    "    print('{} shape'.format(n), t.shape, '{} size'.format(n), t.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "results = {}\n",
    "\n",
    "for C in [10, 100, 1000, 10000]:\n",
    "    print('Creating model with C={}'.format(C))\n",
    "\n",
    "    clf = linear_model.LogisticRegression(C=C, n_jobs=4, solver='liblinear', verbose=10)\n",
    "\n",
    "    %time clf.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "    %time y_pred = clf.predict(X_test)\n",
    "\n",
    "    results[C] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred),\n",
    "    }\n",
    "    \n",
    "    for key, score in results[C].items():\n",
    "        print('{}: {}'.format(key, score))\n",
    "        \n",
    "data = [(C, score['roc_auc']) for C, score in results.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data)\n",
    "plt.plot([d[1] for d in sorted(data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "60 samples (50/50)\n",
    "-------------------\n",
    "\n",
    "(C=.1, n_jobs=1, solver='liblinear')\n",
    "```\n",
    "CPU times: user 0 ns, sys: 10 ms, total: 10 ms\n",
    "Wall time: 5.49 ms\n",
    "Accuracy: 0.637223974763\n",
    "Precision: 0.627218934911\n",
    "Recall: 0.670886075949\n",
    "F1 score: 0.648318042813\n",
    "```\n",
    "\n",
    "120 samples (50/50)\n",
    "-------------------\n",
    "\n",
    "(C=.1, n_jobs=1, solver='liblinear')\n",
    "\n",
    "```\n",
    "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
    "Wall time: 6.98 ms\n",
    "Accuracy: 0.61356466877\n",
    "Precision: 0.611111111111\n",
    "Recall: 0.597444089457\n",
    "F1 score: 0.604200323102\n",
    "```\n",
    "\n",
    "(C=1, n_jobs=1, solver='liblinear')\n",
    "\n",
    "```\n",
    "CPU times: user 0 ns, sys: 20 ms, total: 20 ms\n",
    "Wall time: 10.9 ms\n",
    "Accuracy: 0.652996845426\n",
    "Precision: 0.650485436893\n",
    "Recall: 0.642172523962\n",
    "F1 score: 0.646302250804\n",
    "```\n",
    "\n",
    "(C=10, n_jobs=1, solver='liblinear')\n",
    "\n",
    "```\n",
    "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
    "Wall time: 7.13 ms\n",
    "Accuracy: 0.708201892744\n",
    "Precision: 0.703821656051\n",
    "Recall: 0.70607028754\n",
    "F1 score: 0.704944178628\n",
    "```\n",
    "\n",
    "238 samples\n",
    "-----------\n",
    "\n",
    "(C=16, n_jobs=1, solver='liblinear')\n",
    "\n",
    "```\n",
    "CPU times: user 40 ms, sys: 0 ns, total: 40 ms\n",
    "Wall time: 21 ms\n",
    "Accuracy: 0.695679796696\n",
    "Precision: 0.684848484848\n",
    "Recall: 0.720663265306\n",
    "F1 score: 0.702299564947\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
